{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embedding\n",
        "\n",
        "Word Embedding converts words into meaningful dense vectors so similar words have similar numerical values.\n",
        "\n",
        "# simple (layman) language:\n",
        "\n",
        "gives each word a smart number code so that words with similar meaning get similar codes.\n",
        "\n",
        "# Type of word Embedding\n",
        "\n",
        "1Ô∏è‚É£ Static Word Embeddings\n",
        "\n",
        "Each word has one fixed vector, no matter the sentence.\n",
        "\n",
        "# Popular models:\n",
        "\n",
        "Word2Vec\n",
        "\n",
        "GloVe\n",
        "\n",
        "FastText\n",
        "\n",
        "2Ô∏è‚É£ Contextual Word Embeddings\n",
        "\n",
        "Word vector changes depending on the sentence (context).\n",
        "\n",
        "# Popular models:\n",
        "\n",
        "BERT\n",
        "\n",
        "Transformers\n",
        "\n"
      ],
      "metadata": {
        "id": "k5353x7PcRJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n"
      ],
      "metadata": {
        "id": "Dtie5WDFiRIt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text\n",
        "texts = [\"I love AI\", \"AI is powerful\"]"
      ],
      "metadata": {
        "id": "dlPcfnAWiUT6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenization (words ‚Üí numbers)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n"
      ],
      "metadata": {
        "id": "wRB6_wyRiV2m"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Padding (same length)\n",
        "padded = pad_sequences(sequences, maxlen=4, padding='post')"
      ],
      "metadata": {
        "id": "FUfbLx9wiXiL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vocabulary size (+1 for padding)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "Z7UY219CiZ4q"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Word Embedding Layer\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=8, input_length=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w4YXkc7ibtE",
        "outputId": "ecf98bb5-f790-41f1-84ea-48e4bcb48974"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_output = model.predict(padded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5py89v1id5O",
        "outputId": "7299eed9-cc43-4a2e-c974-5b8e21789b3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 216ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_output.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VAPF1Ehoignp",
        "outputId": "9d9afdbf-5a32-424a-ae6f-e74773d5f7e4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 4, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec\n",
        "\n",
        "word embedding technique that converts words into meaningful numerical vectors based on context.\n",
        "\n",
        "# Simple Idea (Layman)\n",
        "\n",
        "It learns word meaning by looking at surrounding words.\n",
        "\n",
        "# Example:\n",
        "\n",
        "king and queen ‚Üí similar vectors\n",
        "\n",
        "doctor and nurse ‚Üí similar vectors\n",
        "\n",
        "üîÅ Two Types of Word2Vec\n",
        "\n",
        "1Ô∏è‚É£ CBOW (Continuous Bag of Words)\n",
        "Predicts the word using surrounding words.\n",
        "\n",
        "2Ô∏è‚É£ Skip-gram\n",
        "Predicts surrounding words using the main word.\n",
        "\n",
        "# üéØ Why It Is Important\n",
        "\n",
        "Captures meaning\n",
        "\n",
        "Understands similarity\n",
        "\n",
        "Small dense vectors (better than One-Hot)\n",
        "\n",
        "# Core\n",
        "\n",
        "Word2Vec learns word meaning from context and gives similar words similar numerical vectors\n",
        "\n",
        "# ‚ùå Main Problems of Word2Vec\n",
        "\n",
        "1Ô∏è‚É£ No context awareness\n",
        "Same word = same vector always\n",
        "\n",
        "2Ô∏è‚É£ Fails with polysemy (multiple meanings)\n",
        "One word, many meanings ‚Üí cannot handle properly\n",
        "\n",
        "3Ô∏è‚É£ Needs large data\n",
        "Small dataset ‚Üí poor embeddings\n",
        "\n",
        "4Ô∏è‚É£ Out-of-Vocabulary (OOV) problem\n",
        "New/unknown words are not handled well\n",
        "\n",
        "# Word2Vec fails because it cannot understand context and gives one fixed meaning to each word.\n",
        "\n"
      ],
      "metadata": {
        "id": "rcx0wzrre0Ke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding\n",
        "\n",
        "Padding is adding extra zeros to make all sentences the same length.\n",
        "\n",
        "# üß† Why We Need Padding?\n",
        "\n",
        "Neural networks need input of same size.\n",
        "But sentences have different lengths.\n",
        "\n",
        "# Example:\n",
        "\n",
        "‚ÄúI love AI‚Äù (3 words)\n",
        "\n",
        "‚ÄúAI is very powerful‚Äù (4 words)\n",
        "\n",
        "Lengths are different ‚ùå\n",
        "Model cannot process directly.\n",
        "\n",
        "# After Padding (make same length = 5)\n",
        "\n",
        "\"I love AI\"            ‚Üí [I, love, AI, 0, 0]\n",
        "\n",
        "\"AI is very powerful\"  ‚Üí [AI, is, very, powerful, 0]"
      ],
      "metadata": {
        "id": "o1ByDlN-hL3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample sentences\n",
        "texts = [\"I love AI\", \"AI is very powerful\"]"
      ],
      "metadata": {
        "id": "o3g1kXHphj-P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Convert words to numbers (tokenization)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "Pfy4IDSHhmol"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Apply Padding\n",
        "padded = pad_sequences(sequences, maxlen=5, padding='post')\n",
        "\n",
        "#maxlen=5 ‚Üí fixed length of sentence\n",
        "\n",
        "#padding='post' ‚Üí add zeros at end\n",
        "\n",
        "#padding='pre' ‚Üí add zeros at beginning\n"
      ],
      "metadata": {
        "id": "FLUDZDUth3Q2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq5g_-z0h3z8",
        "outputId": "8f2df6f6-3859-48ff-98b6-5d4a6c8ea950"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2, 3, 1, 0, 0],\n",
              "       [1, 4, 5, 6, 0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization (Code)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "St8Ap0e0i7t9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "dkHv3JjFah_K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample sentences\n",
        "texts = [\"I love AI\", \"AI is powerful\"]"
      ],
      "metadata": {
        "id": "2TLry2CNjER4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tokenizer\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "M1dcfs5KjFu-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit on text (learn vocabulary)\n",
        "tokenizer.fit_on_texts(texts)\n"
      ],
      "metadata": {
        "id": "uuJ-eeHMjHDF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to sequences (words ‚Üí numbers)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n"
      ],
      "metadata": {
        "id": "fx6WmY4CjI9K"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s73h1VFljLIE",
        "outputId": "2e58d108-55cd-4a02-ac05-d00f44ca65bb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ai': 1, 'i': 2, 'love': 3, 'is': 4, 'powerful': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsGeKzwKjS8i",
        "outputId": "84f5e8d1-6c48-4a5f-bb00-f9e5ad6c4d5a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[2, 3, 1], [1, 4, 5]]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Sequential Modeling (Very Easy)\n",
        "\n",
        " Sequential modeling means processing data step by step in order, especially for sequence data like text, time series, or speech.\n",
        "\n",
        " Some data comes in order (sequence), and the order matters.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "25rbVYAwkJjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "94la2G36kRI2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Embedding\n",
        "\n",
        "the numerical representation of words (or data) as dense vectors that capture meaning and similarity.\n",
        "\n",
        "It is like giving each word a smart number list that represents its meaning.\n",
        "\n",
        "# Example:\n",
        "\n",
        "king   ‚Üí [0.25, 0.10, 0.90]  \n",
        "queen  ‚Üí [0.27, 0.12, 0.88]  \n",
        "apple  ‚Üí [0.90, 0.05, 0.10]\n",
        "\n",
        "Here:\n",
        "\n",
        "king and queen ‚Üí similar vectors (similar meaning ‚úî)\n",
        "\n",
        "apple ‚Üí different vector (different meaning ‚ùå)\n",
        "\n",
        "# Why We Use Vector Embeddings\n",
        "\n",
        "To convert text into numbers\n",
        "\n",
        "To capture meaning of words\n",
        "\n",
        "To find similarity between words\n",
        "\n",
        "Used in NLP, chatbots, search, translation\n",
        "\n",
        "# üîÑ Difference from One-Hot Encoding\n",
        "\n",
        "One-Hot ‚Üí large, mostly zeros, no meaning\n",
        "\n",
        "Embedding ‚Üí small, dense, meaningful vectors"
      ],
      "metadata": {
        "id": "qqLjG33zkeDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "# Sample sentences\n",
        "texts = [\"I love AI\", \"AI is powerful\"]\n",
        "\n",
        "# 1Ô∏è‚É£ Tokenization (words ‚Üí numbers)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 2Ô∏è‚É£ Padding (same length)\n",
        "padded = pad_sequences(sequences, maxlen=4, padding='post')\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 3Ô∏è‚É£ Create Embedding Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=5, input_length=4))\n",
        "\n",
        "# 4Ô∏è‚É£ Get Embedded Vectors\n",
        "embedded_output = model.predict(padded)\n",
        "\n",
        "print(\"Padded Input:\\n\", padded)\n",
        "print(\"Embedding Shape:\", embedded_output.shape)\n",
        "print(\"Embedding Output:\\n\", embedded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUnpB1-LlSFC",
        "outputId": "a5745032-c9a3-4f24-939f-90f6db59e8ff"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step\n",
            "Padded Input:\n",
            " [[2 3 1 0]\n",
            " [1 4 5 0]]\n",
            "Embedding Shape: (2, 4, 5)\n",
            "Embedding Output:\n",
            " [[[ 0.02852701  0.02125248  0.01269034 -0.02057981  0.02039136]\n",
            "  [-0.03472532 -0.01418555 -0.03664702 -0.0132991   0.04659894]\n",
            "  [-0.03814046 -0.00317582  0.00780234  0.01866535 -0.01538825]\n",
            "  [ 0.03948298  0.04965678 -0.01951586  0.03381972  0.01196776]]\n",
            "\n",
            " [[-0.03814046 -0.00317582  0.00780234  0.01866535 -0.01538825]\n",
            "  [-0.01458217  0.02955754  0.00091927  0.01096587  0.00520628]\n",
            "  [-0.01233997  0.01776362  0.03976815  0.02537883  0.01008726]\n",
            "  [ 0.03948298  0.04965678 -0.01951586  0.03381972  0.01196776]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts words ‚Üí numbers\n",
        "\n",
        "Makes equal length using padding\n",
        "\n",
        "Converts numbers ‚Üí dense vectors"
      ],
      "metadata": {
        "id": "xre_zIZDlUO0"
      }
    }
  ]
}