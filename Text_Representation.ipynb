{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Define\n",
        "\n",
        "Converting words/sentences into numbers so a computer can understand them.\n",
        "\n",
        "# Why do we need ?\n",
        "\n",
        "Computers cannot understand text directly.\n",
        "So we convert text into numbers (vectors) so ML models can process and learn from it.\n",
        "\n",
        "# We use text representation to create good features (numerical vectors) from text so machine learning models can learn better."
      ],
      "metadata": {
        "id": "53r5RIYz28bX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why it is difficult ?\n",
        "\n",
        "It is difficult because language has many meanings and variations, but computers only understand numbers, not words directly."
      ],
      "metadata": {
        "id": "V6TyZ0Hx4dyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What the core idea ?\n",
        "\n",
        "Change text (words) into numbers so a computer can understand and learn from it.\n",
        "\n"
      ],
      "metadata": {
        "id": "9Khqmw9q4nk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What are the technique ?\n",
        "\n",
        "1Ô∏è‚É£ Bag of Words (BoW) ‚Äì Count how many times each word appears\n",
        "\n",
        "2Ô∏è‚É£ TF-IDF ‚Äì Smart counting (gives importance to important words).\n",
        "\n",
        "3Ô∏è‚É£ One-Hot Encoding ‚Äì Each word gets a unique 0 and 1 vector.\n",
        "\n",
        "4Ô∏è‚É£ Word Embeddings ‚Äì Convert words into meaningful dense vectors (like Word2Vec).\n",
        "\n",
        "5Ô∏è‚É£ Contextual Embeddings ‚Äì Understand word meaning based on sentence (like BERT)."
      ],
      "metadata": {
        "id": "qeM70eGc445_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common term ?\n",
        "\n",
        "1Ô∏è‚É£ Corpus\n",
        "\n",
        "A corpus is a collection of text data.\n",
        "\n",
        "2Ô∏è‚É£ Document\n",
        "\n",
        "A single piece of text.\n",
        "\n",
        "3Ô∏è‚É£ Token\n",
        "\n",
        "A token is a small unit of text (usually a word).\n",
        "\n",
        "4Ô∏è‚É£ Tokenization\n",
        "\n",
        "Process of breaking text into tokens (words).\n",
        "\n",
        "5Ô∏è‚É£ Vocabulary\n",
        "\n",
        "List of all unique words in the corpus.\n",
        "\n",
        "6Ô∏è‚É£ Vector\n",
        "\n",
        "Numerical form of text.\n",
        "\n",
        "7Ô∏è‚É£ Feature\n",
        "\n",
        "Numerical representation used by ML model.\n",
        "\n",
        "8Ô∏è‚É£ Stop Words\n",
        "\n",
        "Common words with little meaning.\n",
        "\n",
        "9Ô∏è‚É£ Stemming\n",
        "\n",
        "Cutting words to root form.\n",
        "\n",
        "üîü Lemmatization\n",
        "\n",
        "Converting word to its base meaningful form.\n",
        "\n",
        "\n",
        "# üéØ One-Line Core Memory (Exam/Interview)\n",
        "\n",
        "Corpus = full text data, Document = single text, Token = word, Vocabulary = unique words, Vector = numerical form of text."
      ],
      "metadata": {
        "id": "JlTEMSnB5O9K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LctpGFnuxM6q"
      },
      "outputs": [],
      "source": [
        "# One hot Encoding\n",
        "\n",
        "One-Hot Encoding is a method that converts each word into a vector of 0s and 1s where only one position is 1 and the rest are 0.\n",
        "\n",
        "# Simple\n",
        "\n",
        "Each word gets its own unique position in a list (vocabulary).\n",
        "\n",
        "# Example\n",
        "\n",
        "‚ÄúI love AI‚Äù\n",
        "\n",
        "# Vocabulary (unique words):\n",
        "\n",
        "[I, love, AI]\n",
        "\n",
        "# One-Hot Vectors:\n",
        "\n",
        "I     ‚Üí [1, 0, 0]\n",
        "love  ‚Üí [0, 1, 0]\n",
        "AI    ‚Üí [0, 0, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚ùå Main Problem\n",
        "\n",
        "1Ô∏è‚É£  Vectors become very large if vocabulary is big\n",
        "\n",
        "2Ô∏è‚É£  Does NOT understand meaning or similarity between words\n",
        "\n",
        "3Ô∏è‚É£  Sparsity Problem  (Most values are 0) - Too many zeros ‚Üí inefficient."
      ],
      "metadata": {
        "id": "onFc0czW7LMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words (fixed size) - every sentence is converted into a vector of the same length.\n",
        "\n",
        "Bag of Words is a text representation technique that converts text into numbers by counting how many times each word appears.\n",
        "\n",
        "# Core Idea (Very Simple)\n",
        "\n",
        "Ignore grammar and order.\n",
        "Just count the words.\n",
        "\n",
        "# Example\n",
        "\n",
        "‚ÄúI love AI‚Äù\n",
        "\n",
        "‚ÄúI love coding‚Äù\n",
        "\n",
        "# Step 1: Create Vocabulary (unique words)\n",
        "\n",
        "[I, love, AI, coding]\n",
        "\n",
        "# Step 2: Count word frequency (BoW vectors)\n",
        "\n",
        "\"I love AI\"     ‚Üí [1, 1, 1, 0]  \n",
        "\"I love coding\" ‚Üí [1, 1, 0, 1]\n",
        "\n",
        "**Each number = how many times the word appears.**\n",
        "\n",
        "# ‚ùå Problems with Bag of Words\n",
        "\n",
        "1. Ignores word order (meaning can change)\n",
        "\n",
        "2. Large vocabulary = large vectors\n",
        "\n",
        "3. Does not understand context or meaning (that why we study n-grams)\n",
        "\n",
        "4. Not consider order is an issue\n",
        "\n",
        "# Example:\n",
        "‚ÄúI love AI‚Äù and ‚ÄúAI love I‚Äù ‚Üí Same vector (wrong meaning)\n",
        "\n",
        "# Both give same vector in Bag of Words ‚ùå\n",
        "# But meaning is different.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FwuMEMnc7zJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# n-grams\n",
        "\n",
        "N-grams are sequences of N words used to capture word order and context in text.\n",
        "\n",
        "# Simple Idea\n",
        "\n",
        "Instead of looking at single words, we look at groups of words.\n",
        "\n",
        "# Example\n",
        "\n",
        "‚ÄúI love AI‚Äù\n",
        "\n",
        "1Ô∏è‚É£ Unigram (n = 1) ‚Üí Single words\n",
        "\n",
        "[I], [love], [AI]\n",
        "\n",
        "2Ô∏è‚É£ Bigram (n = 2) ‚Üí 2-word groups\n",
        "\n",
        "[I love], [love AI]\n",
        "\n",
        "3Ô∏è‚É£ Trigram (n = 3) ‚Üí 3-word groups\n",
        "\n",
        "[I love AI]\n",
        "\n",
        "# üéØ Why We Use N-grams\n",
        "\n",
        "1. To capture word order\n",
        "\n",
        "2. To understand context better\n",
        "\n",
        "3. To improve meaning compared to Bag of Words\n",
        "\n",
        "Example:\n",
        "\n",
        "‚Äúnot good‚Äù ‚â† ‚Äúgood‚Äù\n",
        "\n",
        "Bigram captures: [not good] ‚úî (better meaning)\n",
        "\n",
        "# ‚ùå Problems with N-grams\n",
        "\n",
        "Vocabulary becomes very large\n",
        "\n",
        "High memory usage\n",
        "\n",
        "Slower computation\n",
        "\n"
      ],
      "metadata": {
        "id": "lF2hznV1AZK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "\n",
        "TF-IDF = Term Frequency ‚Äì Inverse Document Frequency\n",
        "\n",
        "TF-IDF gives higher importance to important words and lower importance to common words.\n",
        "\n",
        "# Simple Understanding\n",
        "\n",
        "Not all words are important.\n",
        "\n",
        "# Example sentence:\n",
        "\n",
        "‚ÄúThe movie is very very good‚Äù\n",
        "\n",
        "‚Äúthe‚Äù, ‚Äúis‚Äù ‚Üí common (less important)\n",
        "\n",
        "‚Äúgood‚Äù ‚Üí important (more weight)\n",
        "\n",
        "TF-IDF automatically learns this.\n",
        "\n",
        "Two Parts (Easy)\n",
        "\n",
        "1Ô∏è‚É£ TF (Term Frequency)\n",
        "\n",
        "How many times a word appears in a document.\n",
        "\n",
        "Example:\n",
        "\n",
        "‚ÄúAI AI coding‚Äù\n",
        "\n",
        "TF(AI) = 2\n",
        "\n",
        "2Ô∏è‚É£ IDF (Inverse Document Frequency)\n",
        "\n",
        "How rare a word is in all documents (corpus).\n",
        "\n",
        "Common word ‚Üí low IDF\n",
        "\n",
        "Rare word ‚Üí high IDF\n",
        "\n",
        "# Example (Easy)\n",
        "\n",
        "Documents:\n",
        "\n",
        "‚ÄúI love AI‚Äù\n",
        "\n",
        "‚ÄúI love coding‚Äù\n",
        "\n",
        "Common words: ‚ÄúI‚Äù, ‚Äúlove‚Äù ‚Üí lower importance\n",
        "Rare words: ‚ÄúAI‚Äù, ‚Äúcoding‚Äù ‚Üí higher importance\n",
        "\n",
        "So TF-IDF gives more weight to:\n",
        "\n",
        "AI, coding ‚úî\n",
        "\n",
        "# üéØ Why We Use TF-IDF\n",
        "\n",
        "Removes importance of common words\n",
        "\n",
        "Highlights important keywords\n",
        "\n",
        "Better than Bag of Words for many NLP tasks\n",
        "\n",
        "# ‚ùå Limitation\n",
        "\n",
        "Still does NOT understand deep meaning\n",
        "\n",
        "Ignores full context (like BoW)\n",
        "\n",
        "# üìå One-Line (Exam / Interview)\n",
        "\n",
        "TF-IDF is a text representation technique that measures how important a word is in a document relative to the entire corpus.\n",
        "\n",
        "**TF-IDF gives higher weight to important and rare words, and lower weight to common words in the corpus.**"
      ],
      "metadata": {
        "id": "3AJdBNV0CpGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ux5C44x4DfBe"
      }
    }
  ]
}