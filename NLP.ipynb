{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP (Natural Language Processing)\n",
        "\n",
        "A branch of AI that allows computers to understand, interpret, and generate human language\n",
        "\n",
        "Goal: Enable machines to work with text and speech like humans do.\n",
        "\n",
        "# Laymen language ans\n",
        "\n",
        "NLP is teaching computers to understand and work with human language.\n"
      ],
      "metadata": {
        "id": "eINrnxcpfQzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Need for NLP ?\n",
        "\n",
        "1. Huge Amounts of Text Data\n",
        "\n",
        "2. Making Machines Understand Human Language\n",
        "\n",
        "3. Automating Tasks\n",
        "\n",
        "4. Extracting Insights from Data\n",
        "\n",
        "5. Enhancing Human-Computer Interaction"
      ],
      "metadata": {
        "id": "B-IyFgfGf3b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Common NLP Tasks\n",
        "\n",
        "1. Text Classification\n",
        "\n",
        "Goal: Assign a label to a piece of text.\n",
        "\n",
        "2. Sentiment Analysis\n",
        "\n",
        "Goal: Detect emotion or opinion in text.\n",
        "\n",
        "3. Part-of-Speech (POS) Tagging\n",
        "\n",
        "Goal: Identify the role of each word in a sentence.\n",
        "\n",
        "4. Machine Translation\n",
        "\n",
        "Goal: Translate text from one language to another.\n",
        "\n",
        "5. Text Summarization\n",
        "\n",
        "Goal: Summarize long text into short, meaningful content.\n",
        "\n",
        "6. Question Answering\n",
        "\n",
        "Goal: Answer questions based on given text.\n",
        "\n",
        "7. Speech-to-Text / Text-to-Speech\n",
        "\n",
        "8. 2. Named Entity Recognition (NER)\n",
        "\n",
        "Goal: Identify important entities in text, like names, dates, locations.\n",
        "\n",
        "9. Topic Modeling\n",
        "\n",
        "Goal: Find hidden topics or themes in a large set of documents.\n",
        "\n",
        "10. Text Generation\n",
        "\n",
        "Goal: Generate human-like text based on input."
      ],
      "metadata": {
        "id": "AUCwtX7VgMPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Approaches to NLP\n",
        "\n",
        "Deep Learning / Neural Network NLP\n",
        "\n",
        "Modern NLP / Transformer-Based Models\n",
        "\n",
        "Statistical / Machine Learning NLP\n",
        "\n",
        "Rule-Based / Symbolic NLP\n",
        "\n"
      ],
      "metadata": {
        "id": "kd4-9QrtjP_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heuristic Approach\n",
        "\n",
        "basically a smart shortcut or rule of thumb to solve a problem\n",
        "\n",
        "Examples\n",
        "\n",
        "Finding a Store in a New City\n",
        "\n",
        "Spell Check on Your Phone\n",
        "\n",
        "Chatbots\n",
        "\n"
      ],
      "metadata": {
        "id": "DUPUEpy5jkEF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenges in NLP (Natural Language Processing)\n",
        "\n",
        "1. Ambiguity (Words with Multiple Meanings)\n",
        "\n",
        "Problem: Words can have more than one meaning depending on context.\n",
        "\n",
        "2. Sarcasm and Irony\n",
        "\n",
        "Problem: Computers find it hard to detect sarcasm.\n",
        "\n",
        "3. Context Understanding\n",
        "\n",
        "Problem: Words get meaning from other words in a sentence or paragraph.\n",
        "\n",
        "4. Slang, Abbreviations, and Misspellings\n",
        "\n",
        "People write informally online.\n",
        "\n",
        "5. Different Languages and Dialects\n",
        "\n",
        "NLP models trained in one language may not work in another.\n",
        "\n",
        "6. Data Scarcity\n",
        "\n",
        "Modern NLP models need lots of data.\n",
        "\n",
        "7. Ambiguity in Grammar and Syntax\n",
        "\n",
        "Problem: Same sentence can have multiple interpretations.\n",
        "\n",
        "8. Understanding Emotion and Sentiment\n",
        "\n",
        "9. Spelling Error\n"
      ],
      "metadata": {
        "id": "8nLJTD41kANC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Pipeline\n",
        "\n",
        "Think of NLP like making a robot understand human language. You need to clean, break down, and convert text into numbers so a computer can process it.\n",
        "\n",
        "# Set of steps followed to build an end to end NLP software\n",
        "\n",
        "Step 1: Text Collection\n",
        "\n",
        "Step 2: Text Preprocessing (Clean and simplify the text)\n",
        "\n",
        "Step 3: Tokenization\n",
        "\n",
        "Step 4: Text Representation (Word to Numbers)\n",
        "\n",
        "Step 5: Model / Algorithm\n",
        "\n",
        "Step 6: Output / Prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h9R1267ZkxoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Raw Text (Tweets, Reviews)\n",
        "\n",
        "        ‚îÇ\n",
        "        ‚ñº\n",
        "\n",
        "Text Preprocessing\n",
        "(clean text, remove noise)\n",
        "\n",
        "        ‚îÇ\n",
        "        ‚ñº\n",
        "\n",
        "Tokenization\n",
        "(split into words/sentences)\n",
        "\n",
        "        ‚îÇ\n",
        "        ‚ñº\n",
        "\n",
        "Text Representation\n",
        "(convert words to numbers)\n",
        "\n",
        "        ‚îÇ\n",
        "        ‚ñº\n",
        "\n",
        "Model / Algorithm\n",
        "(train or predict)\n",
        "\n",
        "        ‚îÇ\n",
        "        ‚ñº\n",
        "        \n",
        "Output / Prediction\n",
        "(sentiment, spam, translation, chatbot reply)"
      ],
      "metadata": {
        "id": "RLB8ej7ilRPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "\n",
        "Text preprocessing is the step where we clean and prepare raw text so computers can understand it.\n",
        "\n",
        "# Why Needed:\n",
        "\n",
        "Computers don‚Äôt understand messy human text (punctuation, slang, capitalization).\n",
        "\n",
        "Cleaning text improves accuracy of NLP models.\n",
        "\n",
        "Helps computers understand words consistently.(**Computers see text as exact characters, so even a small difference makes them think it‚Äôs a different word.**)\n",
        "\n",
        "# Layman Analogy:\n",
        "\n",
        "Like washing, peeling, and chopping vegetables before cooking."
      ],
      "metadata": {
        "id": "tSDvcP3omatS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lowercasing\n",
        "\n",
        "Lowercasing is the process of converting all letters in text to lowercase.\n",
        "\n",
        "# Why it‚Äôs needed:\n",
        "\n",
        "Computers see ‚ÄúApple‚Äù and ‚Äúapple‚Äù as different words.\n",
        "\n",
        "Lowercasing helps treat the same word uniformly, improving NLP model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "G8qCc6ZKm45g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "\n",
        "text = \"I Love NLP\"\n",
        "\n",
        "text_lower = text.lower()\n",
        "\n",
        "print(text_lower)  # Output: \"i love nlp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J4jQRAmjodt2",
        "outputId": "f619c67c-b16e-4a1e-f026-c12ae82b910b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove HTML Tags ( We use REGEX to remove)\n",
        "\n",
        "HTML tags are the code in a webpage that formats text, like <p>, <a>, <b>, <div>\n",
        "\n",
        "# Why it‚Äôs needed:\n",
        "\n",
        "When you scrape text from websites, you often get tags like <p> or <a href=\"\">.\n",
        "\n",
        "These tags are not meaningful for NLP tasks.\n",
        "\n",
        "Keeping them can confuse the computer."
      ],
      "metadata": {
        "id": "WqLp-Of6nwyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove URLs (Text Preprocessing Step)\n",
        "\n",
        "URLs are links in text, like https://example.com or www.google.com.\n",
        "\n",
        "# Why it‚Äôs needed:\n",
        "\n",
        "URLs are not useful for most NLP tasks like sentiment analysis or text classification.\n",
        "\n",
        "Keeping them can confuse the model.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZusmePFEonIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Remove Punctuation\n",
        "\n",
        "Removing punctuation = cleaning text so NLP models focus on meaningful words, not symbols.\n",
        "\n",
        "# Why it‚Äôs needed:\n",
        "\n",
        "Most punctuation does not carry meaning for NLP tasks like sentiment analysis or text classification.\n",
        "\n",
        "Keeping them can confuse the computer or create unnecessary tokens."
      ],
      "metadata": {
        "id": "_nczmcbOpBNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "text = \"Hello! I love NLP, do you?\"\n",
        "\n",
        "# Remove punctuation\n",
        "clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "print(clean_text)\n",
        "# Output: \"Hello I love NLP do you\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlod8LugpOFM",
        "outputId": "1e27a616-c7fa-40cb-be91-cc2e317f399c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello I love NLP do you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat word statement\n",
        "\n",
        "a sentence or text written in casual chat language, like what people use in texting, social media, or instant messaging.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KDmYjHZ2p95p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_text = \"hey! r u coming 2 the party? lol üòÑ\"\n",
        "\n",
        "# Simple replacement\n",
        "chat_text = chat_text.replace(\"r\", \"are\").replace(\"u\", \"you\").replace(\"2\", \"to\").replace(\"lol\", \"laugh out loud\")\n",
        "print(chat_text)\n",
        "# Output: \"hey! are you coming to the party? laugh out loud üòÑ\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0CEoBb2VqKrE",
        "outputId": "3d9b05d9-1368-4eb7-91ee-2b929639b54f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hey! are you coming to the paarety? laugh out loud üòÑ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spelling Correction\n",
        "\n",
        "Spelling correction is the process of detecting and fixing typos or misspelled words in text\n",
        "\n",
        "# Why It‚Äôs Needed\n",
        "\n",
        "People make typos in chat, reviews, social media, emails.\n",
        "\n",
        "Correcting spelling helps improve NLP model accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "nD9pAznJqP1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I luv NLP and it is amzing!\"\n",
        "corrected_text = str(TextBlob(text).correct())\n",
        "print(corrected_text)\n",
        "# Output: \"I law NLP and it is amazing!\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkOJ5MdXqdxo",
        "outputId": "188e8859-697b-4685-a827-0fd9cd2c529f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I lui NLP and it is amazing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Removing Stop Words\n",
        "\n",
        "Stopwords are common words in a language that don‚Äôt add much meaning to a sentence.\n",
        "\n",
        "# WHY ?\n",
        "\n",
        "Stopwords don‚Äôt add value for many NLP\n",
        "\n",
        "Removing them reduces noise and makes models faster."
      ],
      "metadata": {
        "id": "Vo8FOxnmqi7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "the process of breaking text into smaller pieces called tokens.\n",
        "\n",
        "Tokens can be words, phrases, or sentences depending on the task.\n",
        "\n",
        "# Why It‚Äôs Needed:\n",
        "\n",
        "Computers cannot understand full sentences directly.\n",
        "\n",
        "Breaking text into tokens makes it easier to analyze, count, or process."
      ],
      "metadata": {
        "id": "lvFv8eR1rFG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-Level Tokenization\n",
        "\n",
        "Break a text into individual words (tokens).\n",
        "\n",
        "# Why:\n",
        "\n",
        "Most NLP tasks like sentiment analysis, word counting, or text classification work at the word level\n",
        "\n",
        "# Example\n",
        "\n",
        "Input: \"I love Natural Language Processing!\"\n",
        "Word Tokens: [\"I\", \"love\", \"Natural\", \"Language\", \"Processing\", \"!\"]"
      ],
      "metadata": {
        "id": "2x99OmzHriR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentence-Level Tokenization\n",
        "\n",
        "Break a text into sentences (tokens).\n",
        "\n",
        "# Why:\n",
        "\n",
        "Useful for summarization, translation, or understanding context.\n",
        "\n",
        "Helps analyze one sentence at a time.\n",
        "\n",
        "# Example\n",
        "\n",
        "Input: \"I love NLP. It is amazing!\"\n",
        "Sentence Tokens: [\"I love NLP.\", \"It is amazing!\"]"
      ],
      "metadata": {
        "id": "nQzUY9O3r05d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Why We Need Tokenization\n",
        "\n",
        "Tokenization is the first step in NLP after cleaning text. It‚Äôs essential because computers cannot understand raw text as humans do.\n",
        "\n",
        "Computers Read Numbers, Not Text\n",
        "\n",
        "Easier Analysis\n",
        "\n"
      ],
      "metadata": {
        "id": "xzuQiSGYsL7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n"
      ],
      "metadata": {
        "id": "8u2CEBhps0c6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "text = \"I love Natural Language Processing!\"\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3YcGkors10J",
        "outputId": "88472cce-c33f-4f6b-e71d-7a668a90e19d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokens: ['I love Natural Language Processing!']\n",
            "Word Tokens: ['I', 'love', 'Natural', 'Language', 'Processing', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "the process of reducing words to their root or base form.\n",
        "\n",
        "It removes suffixes like ‚Äú-ing‚Äù, ‚Äú-ed‚Äù, ‚Äú-ly‚Äù, etc.\n",
        "\n",
        "# Why It‚Äôs Needed:\n",
        "\n",
        "Words can have many forms, but they often mean the same thing.\n",
        "\n",
        "Stemming helps NLP models treat similar words as the same."
      ],
      "metadata": {
        "id": "KRhvcOWRtoa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lemmatization (it returns actual dictionary words)\n",
        "\n",
        "process of reducing words to their base or dictionary form (lemma).\n",
        "\n",
        "# Why It‚Äôs Needed:\n",
        "\n",
        "Words have different forms (tense, plural, etc.), but their meaning is the same.\n",
        "\n",
        "Lemmatization helps NLP models understand words correctly and consistently.\n",
        "\n",
        "# Example\n",
        "\n",
        "Words: \"running\", \"ran\", \"better\", \"wolves\"\n",
        "Lemmatized: \"run\", \"run\", \"good\", \"wolf\""
      ],
      "metadata": {
        "id": "ajTg7ksnt7qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code\n",
        "\n"
      ],
      "metadata": {
        "id": "Y1093-FCuQ8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"running\", \"ran\", \"better\", \"wolves\"]\n",
        "\n",
        "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
        "\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f0fZCrauTgm",
        "outputId": "25ef6ba3-92c9-4e21-bf9e-9cd0e7f685e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['run', 'run', 'better', 'wolves']\n"
          ]
        }
      ]
    }
  ]
}